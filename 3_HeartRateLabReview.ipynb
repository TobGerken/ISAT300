{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TobGerken/ISAT300/blob/main/3_HeartRateLabReview.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Rate Lab Review \n",
    "\n",
    "\n",
    "**This notebook is published on my github. It is publicly accessible, but you cannot save your changes to my github. Learning git & github is beyond the scope of this course. If you are familiar with github, you know that to do. If you don't know github, you can save a personal copy of the file to your google drive, so that you can save your changes and can access them at a later date**\n",
    "\n",
    "This notebook is a continuation from previous classes:\n",
    "\n",
    "1. [GettingStarted](https://github.com/TobGerken/ISAT300/blob/main/1_GettingStarted.ipynb)\n",
    "2. [Data Visualization](https://github.com/TobGerken/ISAT300/blob/main/2_DataVisualization.ipynb)\n",
    "\n",
    "\n",
    "## Before we start\n",
    "\n",
    "Some of the code today is going to be a bit complicated and I do not expect you to be able to write code like this on your own. \n",
    "\n",
    "However, we are covering some really important concepts and I expect you to be familiar with the underlying concepts rather than the implementation in Python. For example, we will talk about fitting models to observations from experient and we will explore some of the pitfalls.\n",
    "\n",
    "**I need you to know:** \n",
    "\n",
    "- **what goes wrong with the polynomial models and why**\n",
    "- **understand the components of the exponential model**\n",
    "- **why the final model appears to be a good choice**\n",
    "\n",
    "## Now lets get started \n",
    "\n",
    "The goal of today's exercise is to have a brief look at the Heart Rate data you collected during the lab. In the lab report you are be asked to perform some basic statistics on the data and to fit a model to our observations. \n",
    "\n",
    "In this notebook I will show you how to: \n",
    "\n",
    "- read in data from a google sheet into pandas\n",
    "- perform basic statistics on rows and columns in a pandas data frame\n",
    "- fit and evaluate different mathematical models to experimental data\n",
    "- regognize the importance selecting an appropriate model \n",
    "\n",
    "We have previously covered how make a histogram using pandas. If you don't remember, have a look at the [Data Visualization] lecture (https://github.com/TobGerken/ISAT300/blob/main/2_DataVisualization.ipynb).\n",
    "\n",
    "Because we are still suing pandas we have to import the pandas python module and we make it available with the name `pd`.\n",
    "We will also be using [numpy](https://numpy.org/) a module that extends python for mathematics, science, and statistics. We import this as `np`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this will import pandas and numpy.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a google spreadsheet into a dataframe\n",
    "\n",
    "If you are using google Colab you can easily load data from google sheets, which makes sense since they are both google products. Colab provides the appropriate functions that will authenticate you with google and will then load the google sheet. Doing so will promt a google account logon window. \n",
    "\n",
    "**Note that the below code will only work on Colab! If you chose to work with a different software you can also read a google sheet, but it is more complicated and it might be easier to just download the sheet to your computer as a csv or Excel file that you can then read into a dataframe.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth  # This is the module that allows google to authenticate you\n",
    "import gspread                 # This is the module to interact with a google sheet.  \n",
    "from google.auth import default #autenticating to google\n",
    "auth.authenticate_user()\n",
    "creds, _ = default()\n",
    "gc = gspread.authorize(creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell opens the google sheet specified in the URL, selects the first sheet, \n",
    "# reads the data, and put it in a dataframe\n",
    "gsheets = gc.open_by_url('https://docs.google.com/spreadsheets/d/1Orp3OuRCmQMoJOKCAk3CLwil_H4EzQiEjhvwWxBgeww/edit?usp=sharing')\n",
    "\n",
    "worksheet =gsheets.sheet1\n",
    "\n",
    "# get_all_values gives a list of rows.\n",
    "rows = worksheet.get_all_values()\n",
    "\n",
    "# I am telling python here to use row #1 for the column names and the rest as data\n",
    "df = pd.DataFrame(rows[2:], columns=rows[1])\n",
    "\n",
    "# This bit of code is a bit complicated, but in essence I am telling python to treat all columns as numbers rather than objects\n",
    "cols = list(df.columns) # this produces a list of column names \n",
    "print(cols) \n",
    "df[cols] = df[cols].apply(pd.to_numeric) # I now apply a conversion to all columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am creating some sample data here. \n",
    "# If you have already loaded the data from the google sheet, there is no need to do this. \n",
    "data = {'Subject ID number':[1,2,3,4,5,6], 'Trial1':[64, 74, 66, 84, 64, 72], \n",
    "                          'Trial2':[66, 74, 61, 74, 64, 70], 'Trial3':[64, 72, 61, 74, 69, 71]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that we have a dataframe with 4 colums, one for the Subject ID number and one for each trial. Now we can do some statistics on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "The `describe()` method can always be used to calculate some of the most important descriptive statistics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Howver, there are a lot more available in python. The table below lists the important descriptive statitics functions availabe for a pandas dataframe and their explanations:\n",
    "\n",
    "    count(): The number of non-empty observations.\n",
    "    sum(): The sum of the values.\n",
    "    mean(): The average of the values.\n",
    "    median(): The median of values.\n",
    "    mode(): The mode of the values.\n",
    "    std(): Standard deviation of values.\n",
    "    skew(): The skewness of the values. \n",
    "    kurt(): The kurtosis (also called peakedness) of the value \n",
    "    max(): The largest of the values.\n",
    "    min(): The smallest of the values.\n",
    "    abs(): The absolute value of the values.\n",
    "    prod(): Product of values.\n",
    "    cumsum(): Cumulative sum of values.\n",
    "    cumprod(): Cumulative product of values.\n",
    "    \n",
    "\n",
    "If you don't know what some of these are, I recommend to look them up on the internet. Also here is a [handy cheat-sheet with explanations of some of the most important descriptive statistics](https://res.cloudinary.com/dyd911kmh/image/upload/v1662111933/Marketing/Blog/Descriptive_Statistics_Cheat_Sheet.pdf). It does not contain things like [skewness](https://en.wikipedia.org/wiki/Skewness) though, which can also be really important for describing distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why don't you apply one of these for example the `mean()` to the dataframe df\n",
    "\n",
    "# Complete the code below so that it calculates the statistics for one or more columns\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having three trials for each resting heart rate is good, because it helps us to reduce random error in our measurements and to characterize the underlying uncertainty of the measurand. \n",
    "\n",
    "One way of doing so is to estimate the mean and standard deviation of the resting heart rate measurements for each subject. However, we just noticed that the `mean()` method operates on columns by default. Luckily there is a way of changing this behavior so that it operates on rows instead. \n",
    "\n",
    "I can calculate the average resting heart rate for each subject like this. I also assing this to a new column in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MeanRHR'] = df[['Trial1', 'Trial2','Trial3']].mean(axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `axis` command specifies whether the method should be performed on rows (axis = 1) or colums (axis = 0).\n",
    "\n",
    "Now give it a try and calculate the Standard Deviation (`std()`) of the resting hear rates and save it to a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the code below\n",
    "df['StdRHR'] = \n",
    "# Feel free to add addtional columns for statistics if you would like to. \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also try to create a histrogram of the resting heart rates. \n",
    "# Ask youself, which of the columns should you pick to do so. \n",
    "\n",
    "# Try it here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Rate Revocery \n",
    "\n",
    "We also measured during the lab how your heartrate recovers after exercise. Your recovery time depends on many factors such as your overall pyhiscal fitness, genetics, age, and so on. The rate of Heart Rate Recovery may also provide some information about your health. \n",
    "\n",
    "I am creating a sample dataset for Heart Rate Recovery below and read it into a new dataframe called `df_HRR`. You could also substitute in your own data (or read this in as from a spreadsheet). \n",
    "\n",
    "It is important to realize that you need to supply an appropriate time axis. Here I am assuming that measurements were taken every 20 seconds and to avoid me typping this our, I am using the `np.arange()` function that creates an ascending series of the same length as the heartrate samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create some data\n",
    "heartrates = [187, 152 ,129 ,111 , 99 ,89 ,82,76 ,74, 71, 71, 73, 72, 71]\n",
    "times = np.arange(len(heartrates))*20\n",
    "print('time = %s ' % times)\n",
    "df_HRR = pd.DataFrame(data={'Time':times, 'HeartRate':heartrates})\n",
    "print('Shape of df: (%s, %s)'  % df_HRR.shape)\n",
    "df_HRR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just plot this really quick as a plot of Time vs HearRate . \n",
    "# Remeber how you can do this.\n",
    "# Remember that we have to select the data frame with the Resting Heart Rate (df_HRR) \n",
    "ax1 = df_HRR.plot(kind='scatter', x='Time', y='HeartRate', grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the heartrate decreases with time as we are hoping to see and that there seems to be some time period after which the decrease has leveled off to the resting heart rate. \n",
    "\n",
    "### Modeling Heart Rate Recovery\n",
    "\n",
    "Having a good model, helps us make accurate predictions of a phenomenon. Bad models can be worse than having no model at all, because they lead to decisions with disatrous outcomes. A model is only as good as the underlying data. This is often called *[garbage in garbage out](https://en.wikipedia.org/wiki/Garbage_in,_garbage_out)*. \n",
    "\n",
    "Even more important than having good data itself, is having the expertise to select a good model. A model might fit the experimental data but may show really unexpected behavior in other circumstances. **This is bad!**\n",
    "\n",
    "**Q: Before we continue, what kind of model would you expect to work on this data? You have already conducted the experiement have seen the data and should have made some observations.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can enter your answer here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's step back a minute to talk about models in general**\n",
    "\n",
    "It is good practice to apply the most simple model and often that means a polynomial. \n",
    "\n",
    "Polynomials come in many types, also called orders, with commong polynoical types being linear, quadratic or cubic:\n",
    "![POLYNOMIAL ORDERS](https://editor.analyticsvidhya.com/uploads/349141.png)\n",
    "\n",
    "The higher the order of the polynomial the more curvy it becomes and the easier it is to fit points. \n",
    "\n",
    "You can also fit a polynomial of order 50. This would mean that the equation to be fitted would go up to powers of 50:\n",
    "\n",
    "$y = a_0 + a_1x^1 + a_2x^2 + ... + a_{50}x^{50}$ \n",
    "\n",
    "This means there are 51 coefficients to be estimates ($a_0 \\ldots a_{51}$).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start fitting data. \n",
    "\n",
    "Numpy provides a convenient function (`np.polyfit(x,y,order)`) to fit polynomials to data. We can use this to fit a first order polynomial to the data. \n",
    "\n",
    "Calling `np.polyfit(x,y,1)` with our observed values of time and the desired order (1), will return a set of coeffcients (i.e. the avlues for $a_0$ and $a_1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that x corresponds to time and y corresponds to HeartRate \n",
    "polynomialCoefficients_1 = np.polyfit(df_HRR['Time'],df_HRR['HeartRate'],1)\n",
    "print(polynomialCoefficients_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use another function `np.polyval(<list of coefficients>, <x-values>)` to generate estimates for y at the x values from the polynomial. \n",
    "\n",
    "We can save these to the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HRR['HR_Est_1']=np.polyval(polynomialCoefficients_1,df_HRR['Time'])\n",
    "df_HRR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How about you try this for polynomials of second order (quadratic) and 25th order.**\n",
    "\n",
    "You can always play around with other orders as well and add them to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Complete the Code below to calculate the polynomial coefficients \n",
    "polynomialCoefficients_2  = np.polyfit() #  complete  np.polyfit for a second order polynomial\n",
    "polynomialCoefficients_25 = np.polyfit() #  complete  np.polyfit for a 25th  order polynomial\n",
    "\n",
    "print(polynomialCoefficients_2)\n",
    "print(polynomialCoefficients_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now let's assign our estimated values for the heart rate to the dataframe. \n",
    "\n",
    "# Complete the code below: \n",
    "df_HRR['HR_Est_2']  = np.polyval() # complete np.polyval(<coef>,x) using the 2nd order polynomial coefficients\n",
    "df_HRR['HR_Est_25'] = np.polyval() # complete np.polyval(<coef>,x) using the 25th order polynomial coefficients\n",
    "df_HRR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a plot to see how our fits look. Beatiful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = df_HRR.plot(kind='scatter', x='Time', y='HeartRate', color = 'k')\n",
    "df_HRR[['Time', 'HR_Est_1','HR_Est_2','HR_Est_25']].plot(x='Time', ax=ax1, grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second order and 25th order polynomials seem to fit our data really well. The 25th order polynomical even connects all the measurements ... \n",
    "\n",
    "**Q: Is that something we would expect?**\n",
    "\n",
    "### Evaluating our polynomials\n",
    "\n",
    "We can put a number on our goodness of fit using the square of the correlation coefficient (also known as $R^2$) between the obsered HeartRate and the Estimatated Heart Rate for each of the models. \n",
    "\n",
    "Numpy has a nifty function that will give us the correlation coefficient `np.corrcoef`, which we can then square. Don't worry about the details here. You can always look up the function documentation by using `help(np.corrcoeff)` or in the [numpy manual](https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_1 = (np.corrcoef(df_HRR['HeartRate'],df_HRR['HR_Est_1'])[0,1])**2\n",
    "r2_2 = (np.corrcoef(df_HRR['HeartRate'],df_HRR['HR_Est_2'])[0,1])**2\n",
    "r2_25 = (np.corrcoef(df_HRR['HeartRate'],df_HRR['HR_Est_25'])[0,1])**2\n",
    "\n",
    "print('The 1st order polynomial has an R2 of: %3.2f' % (r2_1))\n",
    "print('The 2nd order polynomial has an R2 of: %3.2f' % (r2_2))\n",
    "print('The 25th order polynomial has an R2 of: %3.2f' % (r2_25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms our initial impression that the higher order polynomials seem to fit the data very well. \n",
    "\n",
    "But there are problems. \n",
    "**Have a look at the quadratic polynomial. What do you notice?**\n",
    "\n",
    "**Do you think that the quadratic fit will provide a good heartrate reading after let's say 2000 seconds?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can even use np.polyval to find out. \n",
    "HRR_Est_2000s = np.polyval(polynomialCoefficients_2,2000)\n",
    "print(HRR_Est_2000s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might be the problem with the very high order polynomial. Well, it has a lot free parameters that can create wiggles, so it fits our data very well, but what happens if we evaluate the heart rate at a higher temporal resolution. \n",
    "\n",
    "Let's say instead of every 20 seconds, we estimate the heartrate every second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_high_res = np.arange(0,262) # this will create a vector of times between 0 and 262 seconds with 1 s resolution. \n",
    "y_high_res = np.polyval(polynomialCoefficients_25,t_high_res) # we now evaluate the higher order polynomical on this \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ax1 = df_HRR.plot(kind='scatter', x='Time', y='HeartRate', grid = True, color = 'k')\n",
    "ax1 = plt.plot(t_high_res,y_high_res) # because this is not in the dataframe I add this to the plot using pyplot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve fitting redux. \n",
    "\n",
    "We now found out that random polynomials are likely not going to do the job. They may fit the observed data well, but they don't represent the underlying process of heart rate recovery and then fail. \n",
    "\n",
    "Here is a comic about curve fitting that illustrates my main points ... (I like [xkcd](www.xkcd.com)).\n",
    "![](https://imgs.xkcd.com/comics/curve_fitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to make sure that the model we fit makes physical sense. This means we need to have an expectation on how the data behaves**. \n",
    "\n",
    "In the case of heart rate recovery, it makes sense to think about an exponential model, where after stopping the exercise,a high heart rate rapidly declines. As the heart rate declines the rate of change will become slower. \n",
    "\n",
    "Exponental decay can be mathematically described like this.\n",
    "\n",
    "$Y = a + b e^{-kt}$ ,\n",
    "\n",
    "where a, b, and k are constants and t is time. \n",
    "\n",
    "This means we have 3 constants that can be changed around for us to find the best fit to our heart rate recovery data.         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Fit\n",
    "\n",
    "I would like you to try to fit the curve manually. This means, play around a bit by selecting values for $a$, $b$, and $k$, untile you find a reasonable fit. \n",
    "\n",
    "I have created some code below that will help you do this. The first part defines a function in python. \n",
    "\n",
    "`\n",
    "def ExpFunction(t, a, b, k):\n",
    "    y = a + b * np.exp(-k*t) \n",
    "    return y\n",
    "`\n",
    "\n",
    "You should remember from your programming class, what a function. In short this function takes for arguments, calculates the value of a function, and returns the result. You shoult notice that this is the same function as the one above. \n",
    "\n",
    "So depending on your choice of variables $a$, $b$, and $k$, you can generate estimates for the heart rate recovery. \n",
    "\n",
    "I also supplied some code for a figure, that shows you the quality of the fit including the $R^2$. $R^2$ is the squared correlation coefficient.\n",
    "\n",
    "An $R^2 = 1.00$ indicates a perfect correlation between two variables. \n",
    "\n",
    "While you play around, try to notice how changing each variable changes the behavior of the curve. \n",
    "\n",
    "**Q: What do you notice?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpFunction(t, a, b, k):\n",
    "    y = a + b * np.exp(-k*t) \n",
    "    return y\n",
    "\n",
    "\n",
    "a = 0        # suggested starting value: 0\n",
    "b = 100       # suggested starting value: 100\n",
    "k = 0.1      # 0.1\n",
    "\n",
    "t = df_HRR['Time']\n",
    "\n",
    "y_est = ExpFunction(t, a, b, k)\n",
    "\n",
    "ax1 = df_HRR.plot(kind='scatter', x='Time', y='HeartRate', grid = True)\n",
    "ax1 = plt.plot(t,y_est, color = 'r')\n",
    "ax1 = plt.text(100, 160, r\"$Y = %3.2f +  %3.2f e^{- %3.2f }$\" % (a,b,k), fontsize = 16)\n",
    "ax1 = plt.text(100, 140, r\"$R^2$ = %3.2f\" % (np.corrcoef(df_HRR['HeartRate'],y_est)[0,1])**2 , fontsize = 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, doing this by hand is tedious (possible, but tedious). You may have reached an $R^2$ of 0.99 or similar. \n",
    "\n",
    "Luckily, the [Scipy](https://scipy.org/) package in python contains a number of algorithms for scientific computing including curve fitting ([Example Here](https://www.geeksforgeeks.org/scipy-curve-fitting/)). \n",
    "\n",
    "If we apply `cureve_fit()`  to our exponential function `ExpFunction` then it returns a set of fitted parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "x = df_HRR['Time']\n",
    "y = df_HRR['HeartRate']\n",
    "# curve fit returns a li\n",
    "fittedParameter, variance = curve_fit(ExpFunction, x, y)\n",
    "print('a= {:.2f}, b = {:.2f},  k = {:.3f}'.format(fittedParameter[0],fittedParameter[1],fittedParameter[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the exponential function go generate estimates for y and plot these. As we can see, the get a great fit to our data. It is almost exactly 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = ExpFunction(x,fittedParameter[0],fittedParameter[1],fittedParameter[2])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ax1 = df_HRR.plot(kind='scatter', x='Time', y='HeartRate', grid = True)\n",
    "ax1 = plt.plot(x,y_est, color = 'r')\n",
    "ax1 = plt.text(100, 160, r\"$Y = %3.2f +  %3.2f e^{- %3.2f }$\" % (fittedParameter[0],fittedParameter[1],fittedParameter[2]), fontsize = 16)\n",
    "ax1 = plt.text(100, 140, r\"$R^2$ = %3.2f\" % (np.corrcoef(y,y_est)[0,1])**2 , fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Curve fitting can be powerful, **when done right!** \n",
    "\n",
    "If done incorrectly, results can be **very misleading**. You should never chose a model, just because it fits well to your observed data. You always need to make sure that the model makes some physical sense and fits your hypothesis about the underlying data. **Know why you are doint the things you are doing!**\n",
    "\n",
    "In Lab 3, you will again see an exponential decay model. Understanding the meaning of the parameters a, b, and k is going to be important. For example, what does $a$ indicatate. What is $b$ (Hint: look at what a + b is equal to?). And what does $k$ do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
